{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e984bd",
   "metadata": {},
   "source": [
    "# Processing the labels of the raw IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed91b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_dir = 'E:/aclImdb/aclImdb/train'\n",
    "labels = []\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c2a68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname),encoding ='utf-8' )\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bbb0d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8c6cf41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adc853b",
   "metadata": {},
   "source": [
    "# Tokenizing the text of the raw IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc8696ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c73175",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "max_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "032bf7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e14a6744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (25000, 100)\n",
      "Shape of label tensor: (25000,)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cd49524",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2f51a7",
   "metadata": {},
   "source": [
    "# Parsing the GloVe word-embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5076128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = 'E:/glove.6B'\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'),encoding = 'utf-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90982872",
   "metadata": {},
   "source": [
    "# Preparing the GloVe word-embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8314998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942990f",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91706f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 100)          1000000   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                320032    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf36c2d3",
   "metadata": {},
   "source": [
    "# Loading pretrained word embeddings into the Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd7e3233",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af01c4",
   "metadata": {},
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3539aecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 2s 168ms/step - loss: 1.1405 - acc: 0.8350 - val_loss: 0.8783 - val_acc: 0.5612\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.8834 - val_acc: 0.5624\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.8857 - val_acc: 0.5622\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.9291 - val_acc: 0.5578\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.9488 - val_acc: 0.5546\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.9019 - val_acc: 0.5624\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.8959 - val_acc: 0.5621\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.9651 - val_acc: 0.5595\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.9916 - val_acc: 0.5595\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 7.8302e-04 - acc: 1.0000 - val_loss: 0.9434 - val_acc: 0.5682\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 8.2393e-04 - acc: 1.0000 - val_loss: 1.0339 - val_acc: 0.5618\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 1s 124ms/step - loss: 4.3465e-04 - acc: 1.0000 - val_loss: 1.0197 - val_acc: 0.5659\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 3.6001e-04 - acc: 1.0000 - val_loss: 1.0776 - val_acc: 0.5608\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 2.3548e-04 - acc: 1.0000 - val_loss: 1.3616 - val_acc: 0.5398\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 0.7742 - acc: 0.8800 - val_loss: 1.9919 - val_acc: 0.5193\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 1.7151 - val_acc: 0.5270\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 8.1799e-04 - acc: 1.0000 - val_loss: 1.5478 - val_acc: 0.5324\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 5.0519e-04 - acc: 1.0000 - val_loss: 1.4304 - val_acc: 0.5381\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 3.5452e-04 - acc: 1.0000 - val_loss: 1.3467 - val_acc: 0.5425\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 2.7006e-04 - acc: 1.0000 - val_loss: 1.2690 - val_acc: 0.5470\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 2.1162e-04 - acc: 1.0000 - val_loss: 1.2138 - val_acc: 0.5537\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 1.8035e-04 - acc: 1.0000 - val_loss: 1.1773 - val_acc: 0.5602\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 1.6045e-04 - acc: 1.0000 - val_loss: 1.1666 - val_acc: 0.5608\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 1.4721e-04 - acc: 1.0000 - val_loss: 1.1536 - val_acc: 0.5624\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 1.3392e-04 - acc: 1.0000 - val_loss: 1.1621 - val_acc: 0.5623\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 1.2158e-04 - acc: 1.0000 - val_loss: 1.1329 - val_acc: 0.5631\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 1.0709e-04 - acc: 1.0000 - val_loss: 1.1874 - val_acc: 0.5608\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 9.3602e-05 - acc: 1.0000 - val_loss: 1.1136 - val_acc: 0.5661\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 8.1136e-05 - acc: 1.0000 - val_loss: 1.1317 - val_acc: 0.5635\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 6.5261e-05 - acc: 1.0000 - val_loss: 1.1634 - val_acc: 0.5644\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 5.3755e-05 - acc: 1.0000 - val_loss: 1.1086 - val_acc: 0.5710\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 4.8855e-05 - acc: 1.0000 - val_loss: 1.2373 - val_acc: 0.5606\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 3.1991e-05 - acc: 1.0000 - val_loss: 1.2076 - val_acc: 0.5624\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 2.2624e-05 - acc: 1.0000 - val_loss: 1.4405 - val_acc: 0.5514\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 1.8070e-05 - acc: 1.0000 - val_loss: 1.2871 - val_acc: 0.5641\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 1.2164e-05 - acc: 1.0000 - val_loss: 1.3374 - val_acc: 0.5617\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 9.3966e-06 - acc: 1.0000 - val_loss: 1.5142 - val_acc: 0.5509\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 7.6917e-06 - acc: 1.0000 - val_loss: 1.3892 - val_acc: 0.5607\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 4.9035e-06 - acc: 1.0000 - val_loss: 1.3055 - val_acc: 0.5678\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 3.3634e-06 - acc: 1.0000 - val_loss: 1.5221 - val_acc: 0.5546\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 0.2837 - acc: 0.9000 - val_loss: 2.0793 - val_acc: 0.5402\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 9.7916e-06 - acc: 1.0000 - val_loss: 2.0780 - val_acc: 0.5402\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 9.7435e-06 - acc: 1.0000 - val_loss: 2.0755 - val_acc: 0.5403\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 9.6497e-06 - acc: 1.0000 - val_loss: 2.0723 - val_acc: 0.5401\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 9.5358e-06 - acc: 1.0000 - val_loss: 2.0685 - val_acc: 0.5406\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 9.3978e-06 - acc: 1.0000 - val_loss: 2.0630 - val_acc: 0.5403\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 9.2055e-06 - acc: 1.0000 - val_loss: 2.0553 - val_acc: 0.5406\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 8.9390e-06 - acc: 1.0000 - val_loss: 2.0430 - val_acc: 0.5415\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 8.5373e-06 - acc: 1.0000 - val_loss: 2.0277 - val_acc: 0.5419\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 8.0631e-06 - acc: 1.0000 - val_loss: 2.0086 - val_acc: 0.5434\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 7.4959e-06 - acc: 1.0000 - val_loss: 1.9813 - val_acc: 0.5439\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 1s 166ms/step - loss: 6.7735e-06 - acc: 1.0000 - val_loss: 1.9493 - val_acc: 0.5457\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 6.0047e-06 - acc: 1.0000 - val_loss: 1.9111 - val_acc: 0.5484\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 5.1931e-06 - acc: 1.0000 - val_loss: 1.8600 - val_acc: 0.5492\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 4.2994e-06 - acc: 1.0000 - val_loss: 1.8105 - val_acc: 0.5531\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 3.5570e-06 - acc: 1.0000 - val_loss: 1.7511 - val_acc: 0.5531\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 2.8626e-06 - acc: 1.0000 - val_loss: 1.6948 - val_acc: 0.5548\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 2.3360e-06 - acc: 1.0000 - val_loss: 1.6267 - val_acc: 0.5585\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 1.8879e-06 - acc: 1.0000 - val_loss: 1.5896 - val_acc: 0.5618\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 1.6897e-06 - acc: 1.0000 - val_loss: 1.5449 - val_acc: 0.5643\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 135ms/step - loss: 1.5256e-06 - acc: 1.0000 - val_loss: 1.5198 - val_acc: 0.5659\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 1.4317e-06 - acc: 1.0000 - val_loss: 1.5040 - val_acc: 0.5674\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 1.3561e-06 - acc: 1.0000 - val_loss: 1.4915 - val_acc: 0.5687\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 1.2717e-06 - acc: 1.0000 - val_loss: 1.5495 - val_acc: 0.5642\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 1.1796e-06 - acc: 1.0000 - val_loss: 1.5332 - val_acc: 0.5648\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 1.0613e-06 - acc: 1.0000 - val_loss: 1.5544 - val_acc: 0.5641\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 9.4558e-07 - acc: 1.0000 - val_loss: 1.5631 - val_acc: 0.5638\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 8.3958e-07 - acc: 1.0000 - val_loss: 1.6336 - val_acc: 0.5593\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 7.3682e-07 - acc: 1.0000 - val_loss: 1.4847 - val_acc: 0.5688\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 6.0821e-07 - acc: 1.0000 - val_loss: 1.4895 - val_acc: 0.5689\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 4.9790e-07 - acc: 1.0000 - val_loss: 1.7063 - val_acc: 0.5577\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 1s 155ms/step - loss: 4.4180e-07 - acc: 1.0000 - val_loss: 1.6145 - val_acc: 0.5620\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 3.3640e-07 - acc: 1.0000 - val_loss: 1.5700 - val_acc: 0.5648\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 2.8449e-07 - acc: 1.0000 - val_loss: 1.6337 - val_acc: 0.5622\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 2.2694e-07 - acc: 1.0000 - val_loss: 1.5258 - val_acc: 0.5698\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 1.9914e-07 - acc: 1.0000 - val_loss: 1.6333 - val_acc: 0.5618\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 1.5001e-07 - acc: 1.0000 - val_loss: 1.6571 - val_acc: 0.5621\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 1.2134e-07 - acc: 1.0000 - val_loss: 1.5720 - val_acc: 0.5701\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 1.0248e-07 - acc: 1.0000 - val_loss: 1.6852 - val_acc: 0.5630\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 8.3305e-08 - acc: 1.0000 - val_loss: 1.7811 - val_acc: 0.5616\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 6.7765e-08 - acc: 1.0000 - val_loss: 1.6980 - val_acc: 0.5637\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 5.3210e-08 - acc: 1.0000 - val_loss: 1.6420 - val_acc: 0.5699\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 4.4386e-08 - acc: 1.0000 - val_loss: 1.8140 - val_acc: 0.5624\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 1s 155ms/step - loss: 3.6498e-08 - acc: 1.0000 - val_loss: 1.7082 - val_acc: 0.5683\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 3.0297e-08 - acc: 1.0000 - val_loss: 1.7611 - val_acc: 0.5636\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 1s 135ms/step - loss: 2.5591e-08 - acc: 1.0000 - val_loss: 1.8147 - val_acc: 0.5628\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 2.1464e-08 - acc: 1.0000 - val_loss: 1.7693 - val_acc: 0.5652\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 1.8772e-08 - acc: 1.0000 - val_loss: 1.7790 - val_acc: 0.5663\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 1.6326e-08 - acc: 1.0000 - val_loss: 1.7571 - val_acc: 0.5686\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 1.5315e-08 - acc: 1.0000 - val_loss: 1.7683 - val_acc: 0.5686\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 1.3927e-08 - acc: 1.0000 - val_loss: 1.8241 - val_acc: 0.5643\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 1.2697e-08 - acc: 1.0000 - val_loss: 1.8106 - val_acc: 0.5658\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 1.1900e-08 - acc: 1.0000 - val_loss: 1.7945 - val_acc: 0.5675\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 1.1463e-08 - acc: 1.0000 - val_loss: 1.7724 - val_acc: 0.5700\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 1.1160e-08 - acc: 1.0000 - val_loss: 1.7804 - val_acc: 0.5700\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 1.1092e-08 - acc: 1.0000 - val_loss: 1.7772 - val_acc: 0.5694\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 1.0613e-08 - acc: 1.0000 - val_loss: 1.7729 - val_acc: 0.5706\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 1.0339e-08 - acc: 1.0000 - val_loss: 1.7967 - val_acc: 0.5707\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 1.0086e-08 - acc: 1.0000 - val_loss: 1.8250 - val_acc: 0.5678\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 8.5223e-09 - acc: 1.0000 - val_loss: 1.7909 - val_acc: 0.5696\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "786b39e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4274364",
   "metadata": {},
   "source": [
    "# Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cdac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "               \n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "               \n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2b9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
